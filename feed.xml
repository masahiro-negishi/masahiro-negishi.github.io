<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://masahiro-negishi.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://masahiro-negishi.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-11-30T12:57:13+00:00</updated><id>https://masahiro-negishi.github.io/feed.xml</id><title type="html">Masahiro Negishi</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Responsible AI</title><link href="https://masahiro-negishi.github.io/blog/2023/workshop-responsible-ai/" rel="alternate" type="text/html" title="Responsible AI"/><published>2023-11-20T00:00:00+00:00</published><updated>2023-11-20T00:00:00+00:00</updated><id>https://masahiro-negishi.github.io/blog/2023/workshop-responsible-ai</id><content type="html" xml:base="https://masahiro-negishi.github.io/blog/2023/workshop-responsible-ai/"><![CDATA[<p>Note: This post is under construction.</p> <h2 id="1-introduction">1. Introduction</h2> <p>With the recent widespread adoption of AI technologies, lively debates have emerged regarding the associated risks. These discussions span a spectrum from concerns about AI replacing jobs to the potential threat of autonomous robots causing harm to humans. However, these debates often remain superficial, particularly among the general public, as terms like “AI” or “Risk” carry vague meanings. Seeking a deeper understanding of the current landscape of AI risks and exploring effective approaches to address them, I participated in an enlightening <a href="https://www.sipri.org/news/2023/sipri-and-unoda-engage-next-generation-ai-practitioners?s=31">educational workshop on Responsible AI for Peace and Security</a> in Malmö, Sweden, held on November 16 and 17, 2023. This post aims to distill the insights gained during the two-day workshop, hoping to contribute to a clearer perspective on the challenges posed by AI risks. This post mainly focuses on AI risks for peace and security.</p> <h2 id="2-ai-risks-on-peace-and-security">2. AI risks on peace and security</h2> <h3 id="definitions-of-peace-and-security">Definitions of peace and security</h3> <p>To begin, let’s delve into the definitions of “peace” and “security.” Both terms carry conventional and contemporary interpretations. Here, we present the definitions of peace:</p> <ul> <li>Conventional peace (negative peace): Denotes the absence of conflicts.</li> <li>Contemporary peace (positive peace): Encompasses the establishment of sustainable societies where individuals experience dignity, equality, and safety.</li> </ul> <p>Now, let’s explore the definitions of security:</p> <ul> <li>Conventional security (objective security): Signifies the state of not being threatened.</li> <li>Contemporary security (subjective security): Involves feeling confident and free from danger.</li> </ul> <p>As evident, significant overlaps exist between peace and security, leading to the standard reference to the two concepts as “peace and security” or P&amp;S in short. One pivotal distinction between conventional and contemporary definitions lies in the fact that the former primarily centers on P&amp;S among nations, whereas the latter extends its scope to individuals. In simpler terms, when we engage in discussions about P&amp;S today, it is common to contemplate peace and security on the individual citizen’s level.</p> <p>More info: <a href="https://www.visionofhumanity.org/defining-the-concept-of-peace/">Defining the Concept of Peace</a></p> <h3 id="definitions-and-categorification-of-risk">Definitions and categorification of risk</h3> <p>Let’s now turn our attention to the definition of risk. According to the Cambridge Dictionary, a risk is the possibility of something bad happening. A risk is comprised of two key components: likelihood and magnitude. The severity of a risk is determined by the extent to which these two components are elevated.</p> <p>Concerning the risks associated with AI, there are three primary types: accidental, misuse, and structural. The table below outlines the definition and provides an example for each type.</p> <table> <thead> <tr> <th>Type</th> <th>Definition</th> <th>Example</th> </tr> </thead> <tbody> <tr> <td>Accidental</td> <td>The potential for unexpected negative outcomes arising from a proper use of AI.</td> <td>An AI recruiting system makes a biased decision that unfairly disadvantages certain groups.</td> </tr> <tr> <td>Misuse</td> <td>The potential for negative consequences resulting from an intentional and malicious use of AI.</td> <td>A deepfake video featuring a politician’s altered speech is disseminated on social media.</td> </tr> <tr> <td>Structural</td> <td>The potential for widespread influence that could be disruptive or harmful.</td> <td>Big corporations dominate the AI market and possess extensive datasets about individuals.</td> </tr> </tbody> </table> <p>Please note that the examples are somewhat connected to P&amp;S: the AI recruiting system undermines equality, the politician’s deepfake video compromises the subject’s dignity and sparks political conflicts, and market domination contributes to the concentration of wealth. As you can see, numerous situations exist where AI undermines P&amp;S. Organizing them into these three types somewhat facilitates a clearer consideration of AI risks.</p> <h3 id="risk-factors">Risk factors</h3> <p>Then, why are there so many AI risks on P&amp;S? Behind this lies a set of risk factors particular to AI. Below, I will enumerate some of them.</p> <ul> <li> <p>Dual use: The same AI technology can be utilized in the military and civilian domains. AI technologies typically advance more rapidly in the private sector, where regulation and control are often more challenging than in the military industry. Developments under lax rules in the private sector may subsequently be repurposed for military use, posing risks.</p> </li> <li> <p>Intangibility: An AI model lacks a physical presence, making it easy to copy and disseminate. While this accessibility benefits everyone, it also increases the potential for misuse.</p> </li> <li> <p>Lack of Understandability: Deep Neural Networks (DNNs) are typically black boxes, challenging even for specialists to comprehend. The probabilistic and opaque nature of DNNs can occasionally lead to accidental risks.</p> </li> <li> <p>Open/Closed Development: Some companies opt to make their models open, while others keep them proprietary and provide APIs. Each approach comes with its own set of advantages and disadvantages. Open-sourcing models enhance transparency and availability but also increase the risk of misuse. On the other hand, closed strategies mitigate misuse risk to some extent but come at the cost of reduced transparency and availability.</p> </li> </ul> <p>Understanding these AI-specific risk factors makes it easier to contemplate countermeasures and can lead to preventing issues beforehand.</p> <h2 id="3-how-to-mitigate-the-ai-risks">3. How to mitigate the AI risks</h2> <h2 id="4-responsible-innovation-of-ai">4. Responsible innovation of AI</h2> <h2 id="5-possible-scenarios">5. Possible scenarios</h2> <h2 id="6-takeaways">6. Takeaways</h2>]]></content><author><name></name></author><category term="AI"/><summary type="html"><![CDATA[What are AI risks and how to mitigate them]]></summary></entry><entry><title type="html">EM algorithm</title><link href="https://masahiro-negishi.github.io/blog/2023/em-algorithm/" rel="alternate" type="text/html" title="EM algorithm"/><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><id>https://masahiro-negishi.github.io/blog/2023/em-algorithm</id><content type="html" xml:base="https://masahiro-negishi.github.io/blog/2023/em-algorithm/"><![CDATA[<h2 id="table-of-contents">Table of Contents</h2> <ul> <li><a href="#1-problem-setting">Problem setting</a></li> <li><a href="#2-background">Background</a></li> <li><a href="#3-algorithm">Algorithm</a></li> <li><a href="#4-reference">Reference</a></li> </ul> <h2 id="1-problem-setting">1. Problem setting</h2> <p>Assume there are latent variables \(z\), which cannot be observed. The goal is to maximize likelihood \(\log p _ {\theta} (x)\), where \(x\) is observed variables. More specifically, given the training set $X$, the objective is to maximize likelihood \(\log p _ {\theta} (X)\). We assume two things to make the problem solvable with EM algorithm:</p> <ul> <li>\(p _ {\theta} (x, z)\) is tractable (For M step)</li> <li>\(p _ {\theta} (z \vert x)\) is tractable (For E step)</li> </ul> <h2 id="2-background">2. Background</h2> <p>You cannot directly maxmize \(\log p _ {\theta} (x)\), thus we consider evidence lower bound (ELBO) as follows:</p> \[\begin{aligned} \log p _ {\theta} (x) &amp;= \log p _ {\theta} (x) \int q(z)dz \\ &amp;= \int q(z) \log \frac{q(z) p _ {\theta} (x, z)}{q(z) p _ {\theta} (z \vert x)}dz \\ &amp;= \int q(z) \log \frac{p _ {\theta} (x, z)}{q(z)}dz + \int q(z) \log \frac{q(z)}{p _ {\theta} (z \vert x)}dz \\ &amp;= \mathcal{L} (q, \theta; x) + D _ {KL} [q(z) \Vert p _ {\theta} (z \vert x)]. \end{aligned}\] <p>You can also derive that \(\mathcal{L} (q, \theta; x)\) is a lower bound of \(\log p _ {\theta} (x)\) by utilizing Jensen’s inequality:</p> \[\begin{aligned} \log p _ {\theta} (x) &amp;= \log \int p _ {\theta} (x, z) dz \\ &amp;= \log \int q(z) \frac{p _ {\theta} (x, z)}{q(z)} dz \\ &amp;\ge \int q(z) \log \frac{p _ {\theta} (x, z)}{q(z)} dz \\ &amp;= \mathcal{L} (q, \theta; x). \end{aligned}\] <h2 id="3-algorithm">3. Algorithm</h2> <p>EM algorithm iterates two steps to maximize \(\mathcal{L} (q, \theta; x) = \log p _ {\theta} (x) - D _ {KL} [q(z) \Vert p _ {\theta} (z \vert x)]\).</p> <h3 id="31-e-step">3.1. E step</h3> <p>E step maximizes \(\mathcal{L} (q, \theta _ t; x)\) in terms of \(q\). This corresponds to minimizing \(D _ {KL} [q(z) \Vert p _ {\theta _ t} (z \vert x)]\) in terms of \(q\). Thus, E step updates \(q(z)\) as follows:</p> \[q(z) = p _ {\theta _ t} (z \vert x)\] <h3 id="32-m-step">3.2. M step</h3> <p>M step maximizes \(\mathcal{L} (q, \theta; x)\) in terms of \(\theta\). Note that \(q(x)\) is set to \(p _ {\theta _ t} (z \vert x)\) in the last E step.</p> \[\begin{aligned} \theta _ {t+1} &amp;\mathrel{\vcenter{:}}= \underset{\theta}{\text{argmax}} \mathcal{L} (p _ {\theta _ t} (z \vert x), \theta; x) \\ &amp;= \underset{\theta}{\text{argmax}} \int p _ {\theta _ t} (z \vert x) \log \frac{p _ {\theta} (x, z)}{p _ {\theta _ t} (z \vert x)}dz \\ &amp;= \underset{\theta}{\text{argmax}} \int p _ {\theta _ t} (z \vert x) \log p _ {\theta} (x, z)dz \\ \end{aligned}\] <p>Note: \(\log p _ \theta (x)\) monotonically increases, thus \(\theta _ t\) will converge to a local optima.</p> <h2 id="4-reference">4. Reference</h2> <ul> <li>Summer seminar “Deep Generative Models” provided by <a href="https://weblab.t.u-tokyo.ac.jp/">Matsuo Lab</a></li> <li><a href="https://academ-aid.com/ml/em">A website provides a mathematical details</a></li> </ul>]]></content><author><name></name></author><category term="ML"/><category term="generative-model"/><summary type="html"><![CDATA[derivation of EM algorithm]]></summary></entry><entry><title type="html">Overview of Generative Models</title><link href="https://masahiro-negishi.github.io/blog/2023/generative-models/" rel="alternate" type="text/html" title="Overview of Generative Models"/><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><id>https://masahiro-negishi.github.io/blog/2023/generative-models</id><content type="html" xml:base="https://masahiro-negishi.github.io/blog/2023/generative-models/"><![CDATA[<h2 id="table-of-contents">Table of Contents</h2> <ul> <li><a href="#1-motivation">Motivation</a></li> <li><a href="#2-learning">Learning</a></li> <li><a href="#3-reference">Reference</a></li> </ul> <h2 id="1-motivation">1. Motivation</h2> <p>We assume that there is a true data distribution \(p_{data}(x)\), which is only accessible through \(\lbrace x_1, x_2, ..., x_N \rbrace\) that are sampled from \(p_{data}(x)\). The goal of generative models is to find an approximation of \(p_{data}(x)\):</p> \[p_{\theta}(x) \approx p_{data}(x).\] <p>A generative model is composed of its architecture and its parameter \(\theta\). The architecture reflects people’s thought on how \(p_{data}(x)\) looks like. Parameter \(\theta\) determines remaining things. There are many applications of generative models including:</p> <ul> <li>generation of new samples</li> <li>abnormal detection, outlier detection</li> <li>denoising, missing value completion</li> </ul> <h2 id="2-learning">2. Learning</h2> <p>As written in Section 1, the goal is to learn \(p_{\theta}(x)\) that approximates \(p_{data}(x)\). There are two issues to achieve this goal.</p> <ul> <li>Issue 1: \(p_{data}(x)\) is unknown</li> <li>Issue 2: It is unclear how to measure the “distance” between \(p_{data}(x)\) and \(p_{\theta}(x)\).</li> </ul> <p>The first issue can be solved by approximating \(p_{data}(x)\) with an empirical distribution \(\hat{p}_{data}^{N}(x) \mathrel{\vcenter{:}}= \frac{1}{N}\sum_{i=1}^{N}\delta(x-x_i)\). The second issue can be solved by introducing KL divergence \(D_{KL}[p(x) \Vert q(x)] \mathrel{\vcenter{:}}= \mathbb{E}_{p(x)}[\log\frac{p(x)}{q(x)}]\). As a result, the learning objective is to derive the following \(\hat{\theta}\):</p> \[\begin{aligned} \hat{\theta} &amp;\mathrel{\vcenter{:}}= \underset{\theta}{\text{argmin}} D_{KL}[\hat{p}_{data}^{N}(x) \Vert p_{\theta}(x)] \\ &amp;= \underset{\theta}{\text{argmin}} \mathbb{E}_{\hat{p}_{data}^{N}(x)}[\log \hat{p}_{data}^{N}(x)] - \mathbb{E}_{\hat{p}_{data}^{N}(x)}[\log p_{\theta}(x)] \\ &amp;= \underset{\theta}{\text{argmax}} \mathbb{E}_{\hat{p}_{data}^{N}(x)}[\log p_{\theta}(x)] \\ &amp;= \underset{\theta}{\text{argmax}} \frac{1}{N} \sum_{i=1}^{N}\log p_{\theta}(x_i) \\ \bigl( &amp;= \underset{\theta}{\text{argmax}} \prod_{i=1}^{N}p_{\theta}(x_i) \bigr) \\ \end{aligned}\] <p>From the above equations, you can understand that minimizing the KL divergence between \(\hat{p} _ {data}^{N}(x)\) and \(p_{\theta}(x)\) is equivalent to maximum likelihood estimation. Thus, in common maximum likelihood estimation, we should keep in mind that we use KL divergence as a distance metric. Due to the assymmetry property of KL divergence, there may be undesirable effects on learned results. In addition, we use \(\hat{p}_{data}^{N}(x)\) instead of \(p_{data}(x)\). Therefore, maximum likelihood estimation does not necessarilly lead to generalization. For example, if the number of training samples $N$ is small, you can fall into over-fitting.</p> <h2 id="3-reference">3. Reference</h2> <ul> <li>Summer seminar “Deep Generative Models” provided by <a href="https://weblab.t.u-tokyo.ac.jp/">Matsuo Lab</a></li> </ul>]]></content><author><name></name></author><category term="ML"/><category term="generative-model"/><summary type="html"><![CDATA[motivation and a basic framework of generative models]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://masahiro-negishi.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://masahiro-negishi.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://masahiro-negishi.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry></feed>